#pragma once
// mpn.hpp - Low-level limb array operations for arbitrary-precision integers
//
// GMP-compatible naming: mpn_add_n, mpn_sub_n, mpn_mul_1, etc.
// All operations are on unsigned limb arrays in little-endian order.
// Uses compiler intrinsics for carry-chain arithmetic (adc/sbb/mulq).

#include <cstdint>
#include <cstdlib>
#include <cstring>
#include <cassert>

#include <immintrin.h>

// ============================================================
// Force-inline / noinline macros
// ============================================================

#if defined(_MSC_VER)
  #define ZINT_FORCEINLINE __forceinline
  #define ZINT_NOINLINE __declspec(noinline)
#elif defined(__GNUC__) || defined(__clang__)
  #define ZINT_FORCEINLINE [[gnu::always_inline]] inline
  #define ZINT_NOINLINE __attribute__((noinline))
#else
  #define ZINT_FORCEINLINE inline
  #define ZINT_NOINLINE
#endif

#ifdef _MSC_VER
#include <intrin.h>
#pragma intrinsic(_addcarry_u64, _subborrow_u64, _umul128, _BitScanReverse64, _udiv128)
#else
#include <x86intrin.h>
#endif

namespace zint {

using limb_t = uint64_t;
using slimb_t = int64_t;
static constexpr int LIMB_BITS = 64;

// ============================================================
// Portable 128-bit arithmetic wrappers
// ============================================================

// 64x64 -> 128-bit multiply: returns low 64 bits, *hi = high 64 bits
inline limb_t umul_hilo(limb_t a, limb_t b, limb_t* hi) {
#ifdef _MSC_VER
    return _umul128(a, b, hi);
#else
    unsigned __int128 r = (unsigned __int128)a * b;
    *hi = (limb_t)(r >> 64);
    return (limb_t)r;
#endif
}

// 128 / 64 -> 64-bit quotient + 64-bit remainder
// Precondition: hi < d (quotient fits in 64 bits)
inline limb_t udiv_128(limb_t hi, limb_t lo, limb_t d, limb_t* rem) {
#ifdef _MSC_VER
    return _udiv128(hi, lo, d, rem);
#else
    unsigned __int128 n = ((unsigned __int128)hi << 64) | lo;
    *rem = (limb_t)(n % d);
    return (limb_t)(n / d);
#endif
}

// Count leading zeros (undefined for x == 0)
inline int clz64(limb_t x) {
#ifdef _MSC_VER
    unsigned long idx;
    _BitScanReverse64(&idx, x);
    return 63 - (int)idx;
#else
    return __builtin_clzll(x);
#endif
}

// ============================================================
// Memory operations
// ============================================================

// Zero n limbs
inline void mpn_zero(limb_t* rp, uint32_t n) {
    std::memset(rp, 0, (size_t)n * sizeof(limb_t));
}

// Copy n limbs, increasing address (safe when rp <= ap)
inline void mpn_copyi(limb_t* rp, const limb_t* ap, uint32_t n) {
    std::memmove(rp, ap, (size_t)n * sizeof(limb_t));
}

// Copy n limbs, decreasing address (safe when rp >= ap)
inline void mpn_copyd(limb_t* rp, const limb_t* ap, uint32_t n) {
    std::memmove(rp, ap, (size_t)n * sizeof(limb_t));
}

// ============================================================
// Bitwise ops (on limb arrays)
// ============================================================

inline void mpn_not_n(limb_t* rp, const limb_t* ap, uint32_t n) {
    uint32_t i = 0;
#if defined(__AVX2__)
    const __m256i ones = _mm256_set1_epi64x(-1);
    for (; i + 4 <= n; i += 4) {
        __m256i a = _mm256_loadu_si256((const __m256i*)(ap + i));
        __m256i r = _mm256_xor_si256(a, ones);
        _mm256_storeu_si256((__m256i*)(rp + i), r);
    }
#endif
    for (; i < n; ++i) rp[i] = ~ap[i];
}

inline void mpn_and_n(limb_t* rp, const limb_t* ap, const limb_t* bp, uint32_t n) {
    uint32_t i = 0;
#if defined(__AVX2__)
    for (; i + 4 <= n; i += 4) {
        __m256i a = _mm256_loadu_si256((const __m256i*)(ap + i));
        __m256i b = _mm256_loadu_si256((const __m256i*)(bp + i));
        __m256i r = _mm256_and_si256(a, b);
        _mm256_storeu_si256((__m256i*)(rp + i), r);
    }
#endif
    for (; i < n; ++i) rp[i] = ap[i] & bp[i];
}

inline void mpn_or_n(limb_t* rp, const limb_t* ap, const limb_t* bp, uint32_t n) {
    uint32_t i = 0;
#if defined(__AVX2__)
    for (; i + 4 <= n; i += 4) {
        __m256i a = _mm256_loadu_si256((const __m256i*)(ap + i));
        __m256i b = _mm256_loadu_si256((const __m256i*)(bp + i));
        __m256i r = _mm256_or_si256(a, b);
        _mm256_storeu_si256((__m256i*)(rp + i), r);
    }
#endif
    for (; i < n; ++i) rp[i] = ap[i] | bp[i];
}

inline void mpn_xor_n(limb_t* rp, const limb_t* ap, const limb_t* bp, uint32_t n) {
    uint32_t i = 0;
#if defined(__AVX2__)
    for (; i + 4 <= n; i += 4) {
        __m256i a = _mm256_loadu_si256((const __m256i*)(ap + i));
        __m256i b = _mm256_loadu_si256((const __m256i*)(bp + i));
        __m256i r = _mm256_xor_si256(a, b);
        _mm256_storeu_si256((__m256i*)(rp + i), r);
    }
#endif
    for (; i < n; ++i) rp[i] = ap[i] ^ bp[i];
}

// ============================================================
// Addition
// ============================================================

// rp[0..n) = ap[0..n) + bp[0..n), return carry (0 or 1)
inline limb_t mpn_add_n(limb_t* rp, const limb_t* ap, const limb_t* bp, uint32_t n) {
    unsigned char carry = 0;
    uint32_t i = 0;
    for (; i + 4 <= n; i += 4) {
        carry = _addcarry_u64(carry, ap[i],   bp[i],   (unsigned long long*)&rp[i]);
        carry = _addcarry_u64(carry, ap[i+1], bp[i+1], (unsigned long long*)&rp[i+1]);
        carry = _addcarry_u64(carry, ap[i+2], bp[i+2], (unsigned long long*)&rp[i+2]);
        carry = _addcarry_u64(carry, ap[i+3], bp[i+3], (unsigned long long*)&rp[i+3]);
    }
    for (; i < n; i++) {
        carry = _addcarry_u64(carry, ap[i], bp[i], (unsigned long long*)&rp[i]);
    }
    return carry;
}

// rp[0..n) = ap[0..n) + b, return carry
inline limb_t mpn_add_1(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
    assert(n > 0);
    unsigned char carry = _addcarry_u64(0, ap[0], b, (unsigned long long*)&rp[0]);
    uint32_t i = 1;
    for (; i < n && carry; ++i) {
        carry = _addcarry_u64(carry, ap[i], 0, (unsigned long long*)&rp[i]);
    }
    if (rp != ap) {
        for (; i < n; ++i) rp[i] = ap[i];
    }
    return (limb_t)carry;
}

// Cleaner version that always copies
inline limb_t mpn_add_1v(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
    assert(n > 0);
    unsigned char carry = _addcarry_u64(0, ap[0], b, (unsigned long long*)&rp[0]);
    for (uint32_t i = 1; i < n; i++) {
        carry = _addcarry_u64(carry, ap[i], 0, (unsigned long long*)&rp[i]);
    }
    return carry;
}

// rp[0..an) = ap[0..an) + bp[0..bn), return carry
// Precondition: an >= bn > 0
inline limb_t mpn_add(limb_t* rp, const limb_t* ap, uint32_t an, const limb_t* bp, uint32_t bn) {
    assert(an >= bn && bn > 0);
    limb_t carry = mpn_add_n(rp, ap, bp, bn);
    if (an > bn) {
        if (carry) {
            carry = mpn_add_1v(rp + bn, ap + bn, an - bn, carry);
        } else if (rp != ap) {
            mpn_copyi(rp + bn, ap + bn, an - bn);
        }
    }
    return carry;
}

// rp[0..n) = ap[0..n) + (bp[0..n) << cnt), return carry limb (0..2^cnt)
// Precondition: n > 0, 0 < cnt < 64. rp may alias ap; rp must not alias bp.
inline limb_t mpn_addlsh_n(limb_t* rp, const limb_t* ap, const limb_t* bp, uint32_t n, unsigned cnt) {
    assert(n > 0 && cnt > 0 && cnt < LIMB_BITS);
    unsigned rcnt = LIMB_BITS - cnt;
    limb_t sh_carry = 0;
    unsigned char carry = 0;
    for (uint32_t i = 0; i < n; ++i) {
        limb_t b = bp[i];
        limb_t shifted = (b << cnt) | sh_carry;
        sh_carry = b >> rcnt;
        carry = _addcarry_u64(carry, ap[i], shifted, (unsigned long long*)&rp[i]);
    }
    return sh_carry + (limb_t)carry;
}

// ============================================================
// Subtraction
// ============================================================

// rp[0..n) = ap[0..n) - bp[0..n), return borrow (0 or 1)
inline limb_t mpn_sub_n(limb_t* rp, const limb_t* ap, const limb_t* bp, uint32_t n) {
    unsigned char borrow = 0;
    uint32_t i = 0;
    for (; i + 4 <= n; i += 4) {
        borrow = _subborrow_u64(borrow, ap[i],   bp[i],   (unsigned long long*)&rp[i]);
        borrow = _subborrow_u64(borrow, ap[i+1], bp[i+1], (unsigned long long*)&rp[i+1]);
        borrow = _subborrow_u64(borrow, ap[i+2], bp[i+2], (unsigned long long*)&rp[i+2]);
        borrow = _subborrow_u64(borrow, ap[i+3], bp[i+3], (unsigned long long*)&rp[i+3]);
    }
    for (; i < n; i++) {
        borrow = _subborrow_u64(borrow, ap[i], bp[i], (unsigned long long*)&rp[i]);
    }
    return borrow;
}

// rp[0..n) = ap[0..n) - b, return borrow
inline limb_t mpn_sub_1(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
    assert(n > 0);
    unsigned char borrow = _subborrow_u64(0, ap[0], b, (unsigned long long*)&rp[0]);
    for (uint32_t i = 1; i < n; i++) {
        borrow = _subborrow_u64(borrow, ap[i], 0, (unsigned long long*)&rp[i]);
    }
    return borrow;
}

// rp[0..an) = ap[0..an) - bp[0..bn), return borrow
// Precondition: an >= bn > 0
inline limb_t mpn_sub(limb_t* rp, const limb_t* ap, uint32_t an, const limb_t* bp, uint32_t bn) {
    assert(an >= bn && bn > 0);
    limb_t borrow = mpn_sub_n(rp, ap, bp, bn);
    if (an > bn) {
        if (borrow) {
            borrow = mpn_sub_1(rp + bn, ap + bn, an - bn, borrow);
        } else if (rp != ap) {
            mpn_copyi(rp + bn, ap + bn, an - bn);
        }
    }
    return borrow;
}

// ============================================================
// Shift
// ============================================================

// rp[0..n) = ap[0..n) << cnt, return bits shifted out from top
// Precondition: 0 < cnt < 64; n > 0
// Direction: processes high to low (safe for rp == ap or rp > ap)
inline limb_t mpn_lshift(limb_t* rp, const limb_t* ap, uint32_t n, unsigned cnt) {
    assert(cnt > 0 && cnt < LIMB_BITS && n > 0);
    unsigned rcnt = LIMB_BITS - cnt;
    limb_t retval = ap[n - 1] >> rcnt;
    for (uint32_t i = n - 1; i > 0; i--) {
        rp[i] = (ap[i] << cnt) | (ap[i - 1] >> rcnt);
    }
    rp[0] = ap[0] << cnt;
    return retval;
}

// rp[0..n) = ap[0..n) >> cnt, return bits shifted out from bottom
// Precondition: 0 < cnt < 64; n > 0
// Direction: processes low to high (safe for rp == ap or rp < ap)
inline limb_t mpn_rshift(limb_t* rp, const limb_t* ap, uint32_t n, unsigned cnt) {
    assert(cnt > 0 && cnt < LIMB_BITS && n > 0);
    unsigned rcnt = LIMB_BITS - cnt;
    limb_t retval = ap[0] << rcnt;
    for (uint32_t i = 0; i + 1 < n; i++) {
        rp[i] = (ap[i] >> cnt) | (ap[i + 1] << rcnt);
    }
    rp[n - 1] = ap[n - 1] >> cnt;
    return retval;
}

// ============================================================
// Comparison
// ============================================================

// Compare ap[0..n) with bp[0..n). Returns -1, 0, or 1.
inline int mpn_cmp(const limb_t* ap, const limb_t* bp, uint32_t n) {
    for (uint32_t i = n; i-- > 0;) {
        if (ap[i] != bp[i])
            return ap[i] > bp[i] ? 1 : -1;
    }
    return 0;
}

// ============================================================
// Normalize
// ============================================================

// Return the number of significant limbs (stripping leading zeros)
inline uint32_t mpn_normalize(const limb_t* ap, uint32_t n) {
    while (n > 0 && ap[n - 1] == 0) n--;
    return n;
}

// ============================================================
// Single-limb multiply
// ============================================================

// rp[0..n) = ap[0..n) * b, return carry limb
inline limb_t mpn_mul_1(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
    limb_t carry = 0;
    for (uint32_t i = 0; i < n; i++) {
        limb_t hi;
        limb_t lo = umul_hilo(ap[i], b, &hi);
        unsigned char c = _addcarry_u64(0, lo, carry, (unsigned long long*)&rp[i]);
        carry = hi + c; // cannot overflow: hi <= 2^64-2, c <= 1
    }
    return carry;
}

// rp[0..n) += ap[0..n) * b, return carry limb
ZINT_FORCEINLINE inline limb_t mpn_addmul_1_scalar(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
    limb_t carry = 0;
    for (uint32_t i = 0; i < n; i++) {
#ifdef __SIZEOF_INT128__
        unsigned __int128 prod = (unsigned __int128)ap[i] * b + carry + rp[i];
        rp[i] = (limb_t)prod;
        carry = (limb_t)(prod >> 64);
#else
        limb_t hi;
        limb_t lo = umul_hilo(ap[i], b, &hi);
        unsigned char c = _addcarry_u64(0, lo, carry, (unsigned long long*)&lo);
        carry = hi + c;
        c = _addcarry_u64(0, lo, rp[i], (unsigned long long*)&rp[i]);
        carry += c;
#endif
    }
    return carry;
}

// ============================================================
// ADX/BMI2 accelerated addmul_1 (optional, runtime-gated)
// ============================================================

#if defined(_MSC_VER) && defined(_M_X64) && defined(ZINT_USE_ADX_ASM)
extern "C" std::uint64_t zint_mpn_addmul_1_adx(std::uint64_t* rp, const std::uint64_t* ap, std::uint32_t n, std::uint64_t b);
#endif

inline bool cpu_has_bmi2_adx() {
#if defined(_MSC_VER) && defined(_M_X64)
    int info[4] = {0, 0, 0, 0};
    __cpuidex(info, 7, 0);
    const bool bmi2 = (info[1] & (1 << 8)) != 0;
    const bool adx  = (info[1] & (1 << 19)) != 0;
    return bmi2 && adx;
#else
    return false;
#endif
}

inline bool cpu_has_bmi2_adx_cached() {
#if defined(_MSC_VER) && defined(_M_X64)
    static const bool has = cpu_has_bmi2_adx();
    return has;
#else
    return false;
#endif
}

// Default implementation: uses ADX kernel when available, otherwise falls back to scalar.
inline limb_t mpn_addmul_1(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
#if defined(_MSC_VER) && defined(_M_X64) && defined(ZINT_USE_ADX_ASM)
    if (n >= 3 && cpu_has_bmi2_adx_cached()) {
        return (limb_t)zint_mpn_addmul_1_adx((std::uint64_t*)rp, (const std::uint64_t*)ap, n, (std::uint64_t)b);
    }
#endif
    return mpn_addmul_1_scalar(rp, ap, n, b);
}

// Explicit name kept for benchmarks/callers; identical to mpn_addmul_1.
ZINT_FORCEINLINE inline limb_t mpn_addmul_1_fast(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
#if defined(_MSC_VER) && defined(_M_X64) && defined(ZINT_USE_ADX_ASM)
    // Keep tiny sizes on the scalar path (branch predictor + call overhead dominates).
    if (n >= 3) return mpn_addmul_1(rp, ap, n, b);
#endif
    return mpn_addmul_1_scalar(rp, ap, n, b);
}

// rp[0..n) -= ap[0..n) * b, return borrow limb
inline limb_t mpn_submul_1(limb_t* rp, const limb_t* ap, uint32_t n, limb_t b) {
    limb_t carry = 0;
    for (uint32_t i = 0; i < n; i++) {
#ifdef __SIZEOF_INT128__
        unsigned __int128 prod = (unsigned __int128)ap[i] * b + carry;
        limb_t plo = (limb_t)prod;
        carry = (limb_t)(prod >> 64);
        unsigned char borrow = _subborrow_u64(0, rp[i], plo, (unsigned long long*)&rp[i]);
        carry += borrow;
#else
        limb_t hi;
        limb_t lo = umul_hilo(ap[i], b, &hi);
        unsigned char c = _addcarry_u64(0, lo, carry, (unsigned long long*)&lo);
        carry = hi + c;
        c = _subborrow_u64(0, rp[i], lo, (unsigned long long*)&rp[i]);
        carry += c;
#endif
    }
    return carry;
}

// ============================================================
// Single-limb division
// ============================================================

// Divide ap[0..n) by d. Store quotient in qp[0..n). Return remainder.
// Precondition: d > 0; ap[n-1] < d (for normalization) or just d != 0
// qp may alias ap.
inline limb_t mpn_divrem_1(limb_t* qp, const limb_t* ap, uint32_t n, limb_t d) {
    assert(n > 0 && d > 0);
    limb_t rem = 0;
    for (uint32_t i = n; i-- > 0;) {
        qp[i] = udiv_128(rem, ap[i], d, &rem);
    }
    return rem;
}

// ============================================================
// Aligned memory allocation
// ============================================================

static constexpr size_t ALLOC_ALIGN = 32; // AVX2

inline limb_t* mpn_alloc(uint32_t n) {
    if (n == 0) return nullptr;
    size_t bytes = (size_t)n * sizeof(limb_t);
    // Round up to alignment
    bytes = (bytes + ALLOC_ALIGN - 1) & ~(ALLOC_ALIGN - 1);
#ifdef _MSC_VER
    return static_cast<limb_t*>(_aligned_malloc(bytes, ALLOC_ALIGN));
#else
    return static_cast<limb_t*>(std::aligned_alloc(ALLOC_ALIGN, bytes));
#endif
}

inline void mpn_free(limb_t* p) {
    if (!p) return;
#ifdef _MSC_VER
    _aligned_free(p);
#else
    std::free(p);
#endif
}

} // namespace zint
